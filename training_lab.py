# -*- coding: utf-8 -*-
"""Training_Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oLZFFmgEtFWu7HfIIhlHBliyGHAjcdQ3

### Imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import kagglehub
import seaborn as sns

"""### Get and Read Data"""

# Download latest version
path = kagglehub.dataset_download("taweilo/loan-approval-classification-data")

print("Path to dataset files:", path)

df = pd.read_csv(path + "/loan_data.csv")
df.head()

df.info()

"""### EDA"""

# Imblaanace test for target columns "credit_score", "loan_status"
print("Imbalance test for credit_score")
credit_score_distribution = df["credit_score"].value_counts()
print(credit_score_distribution)

print("Imbalance test for loan_status")
loan_status_distribution = df["loan_status"].value_counts()
print(loan_status_distribution)

sns.countplot(x="loan_status", data=df)
plt.show()

sns.countplot(x="credit_score", data=df)
plt.show()

"""So we need to use stratified split to fight the imbalance."""

# Check Missing Values
df.isnull().sum()

# Check Duplicate Values
df.duplicated().sum()

# Check for categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns
categorical_columns

df[categorical_columns].nunique()

from sklearn.preprocessing import LabelEncoder

# Label encode categorical columns
label_encoder = LabelEncoder()
for col in categorical_columns:
    df[col] = label_encoder.fit_transform(df[col])

df

# Cheack for different scales
df.describe()

# Take copies of target columns
y_credit_score = df["credit_score"]
y_loan_status = df["loan_status"]

# Use MinMaxScaler to scale the data
from sklearn.preprocessing import MinMaxScaler
features = df.columns.drop(["credit_score", "loan_status"])

scaler = MinMaxScaler()
df[features] = scaler.fit_transform(df[features])
df

"""### Training Models"""

from sklearn.model_selection import StratifiedKFold

"""##### Linear Regression (Target: credit_score)"""

df['loan_status'] = scaler.fit_transform(df[['loan_status']])
features = df.columns.drop(["credit_score"])

X = df[features]
y = df["credit_score"]

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

model = LinearRegression()
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores_mae = []
scores_rmse = []

for train_index, test_index in skf.split(X, y):

    X_Train, X_Test = X.loc[train_index, :], X.loc[test_index, :]
    y_Train, y_Test = y.iloc[train_index], y.iloc[test_index] # Here we don't need the ":" in the columns since y is a Series

    model.fit(X_Train, y_Train)

    y_pred = model.predict(X_Test)

    scores_mae.append(mean_absolute_error(y_Test, y_pred))
    scores_rmse.append(np.sqrt(mean_squared_error(y_Test, y_pred)))

print(f"MAE Score: {np.mean(scores_mae)}")
print(f"RMSE Score: {np.mean(scores_rmse)}\n")

# Calculate the baseline predictions (mean of the target)
baseline_pred = np.full_like(y, y.mean())

# Evaluate the baseline
baseline_mae = mean_absolute_error(y, baseline_pred)
baseline_sqrt = np.sqrt(mean_squared_error(y, baseline_pred))

print(f"Baseline MAE (using mean target): {baseline_mae:.4f}")
print(f"Baseline RMSE (using mean target): {baseline_sqrt:.4f}")

"""We can see here that because the data is normally distributed, the mean is a horrible baseline. Can you guess what else is a horrible baseline?

##### Logistic Regression (Target: loan_status)
"""

# Return the original values before scaling
df['loan_status'] = y_loan_status

# Scale our credit score since it is not the target anymore
df['credit_score'] = y_credit_score
df['credit_score'] = scaler.fit_transform(df[['credit_score']])
df

features = df.columns.drop(["loan_status"])
X = df[features]
y = df["loan_status"]

y.info()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

model = LogisticRegression()
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores_accuracy = []
scores_precision = []
scores_recall = []
scores_f1 = []

for train_index, test_index in skf.split(X, y):

    X_Train, X_Test = X.loc[train_index, :], X.loc[test_index, :]
    y_Train, y_Test = y.iloc[train_index], y.iloc[test_index] # Here we don't need the ":" in the columns since y is a Series

    model.fit(X_Train, y_Train)

    y_pred = model.predict(X_Test)

    scores_accuracy.append(accuracy_score(y_Test, y_pred))
    scores_precision.append(precision_score(y_Test, y_pred, average='weighted'))
    scores_recall.append(recall_score(y_Test, y_pred, average='weighted'))
    scores_f1.append(f1_score(y_Test, y_pred, average='weighted'))

print(f"Accuracy: {np.mean(scores_accuracy):.4f}")
print(f"Precision: {np.mean(scores_precision):.4f}")
print(f"Recall: {np.mean(scores_recall):.4f}")
print(f"F1-Score: {np.mean(scores_f1):.4f}")

from sklearn.metrics import classification_report

# Use the majority class as a baseline
majority_class = y.value_counts().idxmax()
baseline_pred = [majority_class] * len(y)

print(classification_report(y, baseline_pred))

plt.figure(figsize=(16,16))
sns.heatmap(df.corr(), annot=True)

"""We can see from the heatmap that the "previous_loan_defaults_on_file" is very negatively correlated with our target. This means that it should be an important feature for the model for training."""

# Retrieve Logistic Regression coefficients and sort by absolute importance
logistic_importance = list(zip(X.columns, model.coef_[0]))
sorted_logistic_importance = sorted(logistic_importance, key=lambda x: abs(x[1]), reverse=True)

# Extract sorted features and their coefficients
features, coefficients = zip(*sorted_logistic_importance)

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(features, np.abs(coefficients), color='darkblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.title('Logistic Regression Feature Importance')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

"""This confirms or suspicion that even negative correlation is important for machine learning models."""